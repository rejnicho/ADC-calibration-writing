\documentclass[11pt, letterpaper]{article}
%\usepackage[] 

\title{Differential Nonlinearity Analog to Digital Converter Calibration - draft 1}
\author{Renee Nichols, others to come}
\date{February 2024}

\begin{document}
\maketitle 

Talk about the purpose of this work: calibration of the analog to digital converters allows us to understand the digitization of the input signal which are to be used in towards science goals. Mention the results which demonstrate that in fact we have results that are within specifications produced by the manufacturer when we use the calibration technique that we developed. 

\section{Motivations}

*** 
Understanding the analog to digital converters allows us to have an understanding of how the electronics are working within the systems and thus an understanding of the signal received by the telescope from the the sky, which is crucial to all four science themes as put forth by the LSST Book. Rubin Observatory’s LSSTCam uses 18-bit analog to digital converters to record the signal from the bias level all the way to saturation of the silicon CCD sensors. We seek to understand how these electronics work across the entire focal plane - 3024 amplifiers, and verify the specifications set by the manufacturer actually represent the electronics we are using. This is largely motivated by a previous attempt to characterize the ADCs by calculating the differential nonlinearity along bins in the adc and a rise in unbounded integral nonlinearity that was found over the range of the adc when using this original calibration method. 
***

\subsection{Original Method of ADC DNL calibration}
***
In the method developed by A. S. the edges were computed by first computing the differential nonlinearity in each bin, and then using that difference in order to construct the edges. More formally, a distribution of flat pairs was used in order to create a combined distribution to represent the entire range of the adc. As discussed further later, there are some issues with using a run with an inconsistent representation of some adc bins rather than others, but this was not known at the time. The distribution looked like “number of pixels with a specified adc value vs adc value” and created a noisy distribution as a result of being real data. This then needs to be smoothed out in order to keep the long range behavior of the adc but smooth over small deviations bin to bin. In this method, a Savitizy-Golay filter with a window of 65 and order 3 was used in order to smooth the distribution. This filter type was chosen as it was a recommended filter type for smoothing electronic signals. This resulting distribution was assumed to be the expected count distribution if the adc were to be perfect for the index of bin that it was associated with. Therefore, one could take the definition of the differential nonlinearity to be DNL = expected / observed, where the expected is the result of the filter smoothing the true distribution, and the observed is the actual counts from the true distribution. Then it was utilized that the width of adc bins = 1 - dnl, and the bins were placed accordingly. This allowed the calibration of the adc over a range of approximately 26-100k adu, which was deemed sufficient for our science use. 
*** 

Note: I also don't think this is accurate 100 percent, since I believe that A used the filter on every single image and then constructed edges as a combination of each exposure and did some complicated stuff to make this workl... you might want to expand on this... 

\subsection{Differential and Integral Nonlinearity blow up}
**** The analog to digital converter’s purpose is to convert an analog signal to a digital one that can be readout and stored for further use. A range of analog signals is to be converted into the bins of the adc. In a perfect adc, the bins would be integer spaced apart, with integer analog signal mapped directly to integer digital signal. In reality, there are some deviations of these edges, resulting in the digital edges of the adc being non-integer values. In order to understand the output signal, we want to be able to determine the location of these edges and look for any trends that may be present in them. The edges together form the adc bins, which are what the analog signal is mapped onto. There are two quantities that we may be particularly interested in when looking at the bins and their edges, called the integral nonlinearity (INL) and the differential nonlinearity (DNL). 
The INL is the deviation of the middle of the true bin from the middle of an ideal middle. Thus if the bins begin to deviate significantly from an ideal adc, we will see our components of the inl steadily take off. The specifications sheet determines that for any adc, there should be an inl maximum of +/- 2 for any bin, with a typical value being +/- 1. 
The DNL is the difference of any one bin from having a width of 1 along the adc. Thus is there are small or large on average bins, this will show up as having some non zero DNL. The specifications sheet deems that any bin should have a typical +/-0.5, with the maximum being -0.85 or +1.5, meaning there should be no skipped bins along the adc. 
The original method had both an inl and dnl blow up as a result of the method. As explained earlier, the method utilized a run of flat pairs where there was significant signal in the early range of the adc, and an exponential decay of signal outside that region. Thus, in the early region when we applied the filter, this was taken to be the expected signal in this region, and there was significantly larger DNL in that region, which depending on the location of the first binning, could be greater than abs(1), suggesting that bins in this region were skipped, while others demonstrated being extremely small, with widths on the order of 0.1 adu. This problem could be avoided when one threw out the data in entirely in the low region of the adc and began the binning method on bins with more consistent data, meaning that there was a dnl dependence on the starting point of the method. 
There was also an issue when looking at the INL using this calibration method. When using this method we found that there was an unbounded inl over the region of the adc, which resulted in there being a deviation of over 70 adu from an ideal adc. This suggested that this adc was extremely out of specification. This was also minimized to the order 10 adu when terminating portions of the distribution, but no amount of selective cutting of the distribution forced both the dnl and inl to fall within the specifications. This further motivated the need to try a new method for calibration, as this large of a deviation from the specifications smelled of an artifact of the method rather than a smoking gun that something was severely wrong with the adc. 
**** 

\section{Process}
*** After examining the previous method of calibrating the ADC, we see that a need exists for a calibration of the ADC that represents the ADC itself and does not input INL or DNL which is an artifact of the method rather than being representing the true behavior of the electronics. In this, we will still use entire runs for the calibration, but be more selective on which portions we wish to use and how to use all exposures when some regions of the adc are more represented than others as well as look at the advantages of using “ramp” type runs which aim to uniformly probe the entire range of the adc with their choices of exposures. We will then more carefully consider and craft what the signal in the runs in the result from the filter yield, including examining and constructing the probability distribution function (pdf) or the distribution of counts across pixels and using this to carefully construct the bin edges along the full range of the adc. I will also detail important things to not miss when creating a calibration, including removing bins that are occupied only by noise and protecting the distribution from any non-occupied bins. We also include a repository for other’s use. 

\subsection{Choosing runs}
***
Calibration of the adc means we need signal from the ccd over the range of the adc, which is roughly from bias through saturation, which is mapped to roughly 100k so called adc “codes” with unit ADU (analog to digital unit). There is some distribution of how the signal is expected to fall on along the adc, which is called the probability distribution function, which signals the amount of signal expected to fall in each bin. We expect each bin to have a different amount, hence this is a distribution of how the signal is expected to fall. Since electronics behave individually, we have no a-priori knowledge of this distribution and instead need to examine the signal from the adc that we do have access to. 

Another aspect to consider when determining the run to use is how the calibration responds to the features present in the combined distribution. Through extensive work, we have been able to determine that this calibration method does an excellent job to represent the input data present, including periodic features along the adc and any over or under represented regions of the adc. When utilizing a distribution where there is a large amount of exposures in which fall in a single region, and few in another, the region with more signal creates small on average bins, with large amounts of dnl and an unstable integral nonlinearity. (See appendix more about unstable INL). When comparing this to using this calibration when all of the adc is relatively consistently represented, we yield dnl and inl within the specifications on from the manufacturer. This of course is expected, as an over or under representing of a region of the adc will affect the model of our probability distribution function and hence change how our calibration understands these bins. Thus it is very important to have a distribution where all portions of the adc are equally represented, otherwise we are introducing features and substructure of the adc that simply do not exist. 

These two requirements of the entire ADC to be represented is not sufficient to characterize the adc. Since each amplifier is composed for over 1 million pixels and each pixel is responding and yielding a signal that is proportional to the amount of light incident on the sensor, trying to characterize the adc over the entire range is functionally impossible, as only a single subregion of the adc would be represented in any single exposure. Therefore we need to determine the groupings of exposures which would allow us to accurately see the distribution of signal over the entire adc. Of course, a run of exposures with increasing exposure time with continuous readout would satisfy this requirement, as we could see how the adc responds over the full range of saturation. This is shown and done in run 6b 13549 as part of eo testing. Yet it is not always practical to request a special run in future cases or to further verify this calibration, as this single run for calibration took over 6 hours (check this). Instead it is very beneficial to be able to also use flat pair runs, which still record exposures from bias to saturation of the adc, but have an uneven distribution of the adc which is represented. We can do this by dynamically pre-scaling an entire run of flat pairs. 

The prescale method works by combining all the exposures from a flat pair run into a combined distribution. This distribution will be extremely right skewed, with a majority of exposures being in under 30k ADU region. Then looking at higher adc codes, we choose some saturation level we wish to represent, for 13144 we choose 5k counts represented in each bin. We then compute a prescale factor for each subregion of the adc (with length 25 adu), where prescale factor = 5k/ meancount of 25 bins. Then when we re-pull an exposure, we determine the location along the adc of the image's peak and use that to determine the appropriate prescale factor. We then determine the number of pixels to save from that exposure by the following formula: number pulled n = prescale factor*number pixels in amplifier (numbers for e2v and itl separate). We then use a randomization process to select n pixels from the original distribution. We can repeat this for each exposure in the run, and built up a psuedo-flat distribution over the full range of the adc that is not negatively impacted by more exposures in one region over another. When comparing the results from a prescaled run and the ramp run, we note no significant deviations in the calibration bins, suggesting this does a sufficient job in repurposing other types of eo runs for this calibration. 
***

\subsection{Determination of ADC edges}

*** 
Calibration of the ADC requires sufficient understanding of the distribution of counts of pixels with a recording of a specific adu value vs that adu value. We can state that we understand each bin of the adc can be thought of as the integral of the pdf from the left edge of the bin to the right edge of the bin, and will have the corresponding number of counts as associated with the amount. Both the pdf and the left and right edges of each of the bins are unknown to us, with knowing the edges being most important as this would yield a calibration of the adc. Thus we can also delve deeper into the meaning of a filtered distribution, namely that we can state that there is a relationship between the filtered distribution values and the pdf as well, filtered value is equivalent to the integral of the pdf from the integer left edge of the pdf over a region of width 1. This is justified by filters’ smoothing over the subregions of the adc but preserving the long range behavior of the adc. In this calibration we change the window of the Savitzky-Golay filter to being 33 instead of 65 (insert the reason for this when you remember it). This still preserves the long range behavior of the combined distribution while smoothing over small deviations bin to bin. With this, we can utilize the fundamental theorem of calculus to examine how an integral of the pdf behaves over larger regions of the adc. Thus we can take a cumulative sum of progressively more filtered values to see the trend of the integral of the pdf over the range of the adc. Since our ultimate goal is to be able to use this in order to determine the edges of the adc, we want to be able to interpolate non-integer values of this integral. To do this, we can fit a cubic spline to the cumulative sum, which allows us to evaluate the integral of the pdf over small subregions of the adc bins. Thus we can decompose the integral of the observed distribution to be observed counts = spline(right) - spline(left). Given we choose the first left edge to be our earliest populated bin, this choice preserves as much of the adc available in order to calibrate as much of the range as possible. Therefore we have one equation and one unknown, so we need to utilize the spline and determine where observed counts + spline(left) = spline(right). 
The method developed, we utilize a convergence method, which allows us to first assume a bin will have a width of 1, then check if this right edge is more or less than expected, then we increase/decrease the bin size by a standard amount and recheck the value. This process is repeated until a convergence interval is reached, typically +/- 1 count. For a bin with an observed depth of over 5000, this is less than 0.2 of a percent difference, which from many iterations and trials of varying convergence intervals this one yields sufficient results in a reasonable timeframe. (rate of edge making I believe it is 8s/100k bins). 
Altogether, this yields a calibration of a single adc in under 10 seconds. This makes no assumptions about the DNL and INL in its computation, but rather utilizes the combined distribution of the amplifier and a filter in order to recover the edges of the adc. 
*** 

\subsection{Shortcomings}
***
In the calibration process, some issues arose with minute fixes implemented in order to not throw off the entire calibration process. The first of which is the implementation of the spline of the cumulative sum of the filtered values. Since the spline is a cubic function, it has the potential to interpolate and yield a negative value. This is an unphysical solution, what exactly would it mean for a region of the adc to yield a negative number of counts of a specific adu value? In order to safeguard against this, we use a padding function, which instead of allowing negative values, inputs a value of 0. This inherently protects the spline from yielding negative values. 
Even more rudimentary than this, is pulling out any bins from the distribution with less than 100 counts in them. These have signals that are 1/50 of a typical bin, and can be viewed as noise from the adc. We cut these bins to differentiate between noise and true behavior of the adc, as we wish to model the adc and not noise of the distribution. We also take care into considering whether the combined distribution yields any locations along the adc with a skipped bin. Since bins are removed from the distribution which are classified as noise, this leads to the possibility of missing codes. In order to prevent this, the method takes into consideration the location of a skipped bin, and recommends that the calibration begins after a skipped bin. The final calibration also returns the location of the first bin used, in order to compare from amplifier to amplifier where we start the calibration process. This not only preserves the behavior of the adc, but prevents the accidental termination of the method or interpolation over a bin with insufficient data for calibration. 
Another concern arising from the method is the “residual” or the convergence interval to determine the right edge. As discussed earlier, we allow for a custom convergence interval, which for this work we take to be 1 count. We state that while you could require this convergence to be more strict, we are wary to dictate that the converge shouldn’t be more precise than the rest of the measurements, and thus having a more precise measurement than this (1count/5000), would imply the measurements are more accurate than they truly are. An attempt to reduce residuals by adding/removing the last count from the next bin yielded dnl and inl values not different than ignoring the uncertainty, which suggests such a treatment is unnecessary. 
The last concerns in this process relates exactly to the custom ramp run 13549 which was used as one of the demonstration calibrations. In this, the distributions from bias to saturation in some of the adcs have a sort of peak to them either near the bias or near the saturation, which have a width that is greater than a simple pile up of higher or lower signal. This was troubling to deal with, as the distributions in most amplifiers did not have this, and there seemed to be no correlation between amplifiers with this feature, and those without. Thus as a work around, and because of the dependence of injected inl and dnl from sharp features such as this as discussed earlier, the choice to terminate the distribution to exclude these features was made. The termination was all bins within 50 of the peak of this distribution. Effectively for each peak about 100 bins were removed (100 over the range of 100k, is less than 1 percent). This allowed us to continue to use the calibration method developed and avoid pitfalls of awkward data collection. 
The final problem we encountered was a raft issue in the R4X region of the focal plane in run 13549, which had more digitizations than expected in every single exposure in the run. This means that there were measurements for over 7k “pixels” above that which are expected for ITL sensors. This was only observed in the 3 rafts in this column, although the same method for pulling the images was also utilized in for all amplifiers on all rafts, suggesting something is amiss. Since a reason for these extra digitizations could not be found, we note this deviation but continued with the calibration process anyways. 
*** 

\section{Conclusion}
In this section I will discuss the results of both using a ramp run and a pre-scaled flat pair run in order to determine the adc edges. I will show that both runs yield very similar results. We will have a few plots which show for the entire focal plane the calibration. 

\subsection{GitHub Access}
***
To access a version of the code used to create these results and use the GitHub repository rejnicho/
Focal-Plane-DNL-adc-Calibration, and follow the three step process to access, calibrate, and yield plots. 
***

\section{Future Work}
Discuss about how it will be beneficial to improve the method slightly by using better parallel processing and more cores to make the process more efficient. Also could discuss science-based applications of this work, including the DESC stuff I have upcoming.

\section{Appendix}
In this section I want to list the various aspects and things that I put time into, only for it to turn out as a dead end. 

a. more about unstable INL - you just verified it existed, but talk about how each binning location made a different inl which was correlated where on the bad distribution binning started. (When starting the calibration method in any given region of the adc, there would be wildly different responses in the inl. ) 


\end{document}
